{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Agent with Web Search\n",
    "\n",
    "\n",
    "This notebook is based in the demo published by https://github.com/opendatahub-io/llama-stack-demos/\n",
    "\n",
    "\n",
    "This notebook will introduce how to build a simple agent using Llama Stack's agent framework, enhanced with a single tool: the builtin web search tool. This capability will  allow the agent to retrieve up to date external information beyond the limits of its training data. This is an important step toward developing a more capable and autonomous agent.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial will walk you through how to build your own AI agent who can search the web:\n",
    "\n",
    "1. Configure a Llama Stack agent.\n",
    "2. Enhance the agent by providing it access to a specific tool\n",
    "2. Interact with the agent and tests its use of the web search tool.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this notebook, ensure that you have:\n",
    "- Followed the instructions in the [Setup Guide](./Level0_getting_started_with_Llama_Stack.ipynb) notebook. \n",
    "- A Tavily API key. It is critical for this notebook to run correctly. You can register for one at [https://tavily.com/](https://tavily.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "We will start with a few imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](./Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n",
      "Inference Parameters:\n",
      "\tModel: llama3.2:3b-instruct-fp16\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 512}\n",
      "\tstream: True\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id =os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"False\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure an agent for tool use.\n",
    "\n",
    "- **Agent Initialization**: First we create an `Agent` instance with the desired LLM model, agent instructions and tools.\n",
    "\n",
    "- **Instructions**: The `instructions` parameter, also referred to as the system prompt, specifies the agent's role and behavior. In this example, the agent is configured as a helpful web search assistant. It is instructed to use a tool whenever a web search is required and to respond in a friendly and helpful tone.\n",
    "\n",
    "- **Tools**: The `tools` parameter defines the tools available to the agent. In this case, the `builtin::websearch` tool is used, which enables the agent to perform web searches. This tool is essential for retrieving up-to-date information from the web.\n",
    "\n",
    "- **How It Works**: When a user query is provided, the agent processes the input and determines whether a tool is required to fulfill the request. If the query involves retrieving information from the web, the agent invokes the `builtin::websearch` tool. The tool interacts with Tavily Search to fetch real-time data, which is then processed and returned to the user in a friendly and helpful tone. This workflow ensures that the agent can handle a wide range of queries effectively.\n",
    "\n",
    "For more details on the `builtin::websearch` tool and its capabilities, refer to the [Llama-stack tools documentation](https://llama-stack.readthedocs.io/en/latest/building_applications/tools.html#web-search-providers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/tools?toolgroup_id=builtin%3A%3Awebsearch \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client, \n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are a helpful websearch assistant. When you are asked to search the latest you must use a tool. \n",
    "            Whenever a tool is called, be sure return the response in a friendly and helpful tone.\n",
    "            \"\"\" ,\n",
    "    tools=[\"builtin::websearch\"],\n",
    "    sampling_params=sampling_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the agent.\n",
    "- Populate `user_prompts` with questions that you would like to ask the agent.\n",
    "- Create a unique agent session for this conversation so that it can store metadata and context history in the Llama Stack server.\n",
    "- Finally, display the agent's responses for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/78c05433-ee0f-4152-ae91-97ba8665a4d2/session \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/agents/78c05433-ee0f-4152-ae91-97ba8665a4d2/session/b9144673-2d01-4754-8983-d8785171b33a/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\u001b[34mProcessing user query: Quienes son los hijos de Kirk Dougles?\u001b[0m\n",
      "==================================================\n",
      "\u001b[33minference> \u001b[0m\u001b[33mP\u001b[0m\u001b[33mued\u001b[0m\u001b[33mo\u001b[0m\u001b[33m ayud\u001b[0m\u001b[33marte\u001b[0m\u001b[33m con\u001b[0m\u001b[33m eso\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Para\u001b[0m\u001b[33m obtener\u001b[0m\u001b[33m la\u001b[0m\u001b[33m información\u001b[0m\u001b[33m más\u001b[0m\u001b[33m rec\u001b[0m\u001b[33miente\u001b[0m\u001b[33m sobre\u001b[0m\u001b[33m los\u001b[0m\u001b[33m hijos\u001b[0m\u001b[33m de\u001b[0m\u001b[33m Kirk\u001b[0m\u001b[33m Douglas\u001b[0m\u001b[33m,\u001b[0m\u001b[33m te\u001b[0m\u001b[33m recom\u001b[0m\u001b[33miendo\u001b[0m\u001b[33m buscar\u001b[0m\u001b[33m en\u001b[0m\u001b[33m una\u001b[0m\u001b[33m fu\u001b[0m\u001b[33mente\u001b[0m\u001b[33m conf\u001b[0m\u001b[33miable\u001b[0m\u001b[33m como\u001b[0m\u001b[33m Wikipedia\u001b[0m\u001b[33m o\u001b[0m\u001b[33m un\u001b[0m\u001b[33m artículo\u001b[0m\u001b[33m de\u001b[0m\u001b[33m noticias\u001b[0m\u001b[33m actual\u001b[0m\u001b[33mizado\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Aqu\u001b[0m\u001b[33mí\u001b[0m\u001b[33m te\u001b[0m\u001b[33m de\u001b[0m\u001b[33mjo\u001b[0m\u001b[33m algunos\u001b[0m\u001b[33m resultados\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33mSeg\u001b[0m\u001b[33mún\u001b[0m\u001b[33m Wikipedia\u001b[0m\u001b[33m,\u001b[0m\u001b[33m Kirk\u001b[0m\u001b[33m Douglas\u001b[0m\u001b[33m tiene\u001b[0m\u001b[33m tres\u001b[0m\u001b[33m hijos\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Michael\u001b[0m\u001b[33m Douglas\u001b[0m\u001b[33m (\u001b[0m\u001b[33mn\u001b[0m\u001b[33mac\u001b[0m\u001b[33mido\u001b[0m\u001b[33m el\u001b[0m\u001b[33m \u001b[0m\u001b[33m25\u001b[0m\u001b[33m de\u001b[0m\u001b[33m sept\u001b[0m\u001b[33miembre\u001b[0m\u001b[33m de\u001b[0m\u001b[33m \u001b[0m\u001b[33m194\u001b[0m\u001b[33m4\u001b[0m\u001b[33m)\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Joel\u001b[0m\u001b[33m Douglas\u001b[0m\u001b[33m (\u001b[0m\u001b[33mn\u001b[0m\u001b[33mac\u001b[0m\u001b[33mido\u001b[0m\u001b[33m el\u001b[0m\u001b[33m \u001b[0m\u001b[33m24\u001b[0m\u001b[33m de\u001b[0m\u001b[33m jun\u001b[0m\u001b[33mio\u001b[0m\u001b[33m de\u001b[0m\u001b[33m \u001b[0m\u001b[33m194\u001b[0m\u001b[33m7\u001b[0m\u001b[33m)\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Peter\u001b[0m\u001b[33m Douglas\u001b[0m\u001b[33m (\u001b[0m\u001b[33mf\u001b[0m\u001b[33malle\u001b[0m\u001b[33mcido\u001b[0m\u001b[33m el\u001b[0m\u001b[33m \u001b[0m\u001b[33m6\u001b[0m\u001b[33m de\u001b[0m\u001b[33m fe\u001b[0m\u001b[33mbrero\u001b[0m\u001b[33m de\u001b[0m\u001b[33m \u001b[0m\u001b[33m200\u001b[0m\u001b[33m0\u001b[0m\u001b[33m)\n",
      "\n",
      "\u001b[0m\u001b[33mEs\u001b[0m\u001b[33m importante\u001b[0m\u001b[33m tener\u001b[0m\u001b[33m en\u001b[0m\u001b[33m cuenta\u001b[0m\u001b[33m que\u001b[0m\u001b[33m la\u001b[0m\u001b[33m información\u001b[0m\u001b[33m puede\u001b[0m\u001b[33m vari\u001b[0m\u001b[33mar\u001b[0m\u001b[33m depend\u001b[0m\u001b[33miendo\u001b[0m\u001b[33m de\u001b[0m\u001b[33m la\u001b[0m\u001b[33m fu\u001b[0m\u001b[33mente\u001b[0m\u001b[33m y\u001b[0m\u001b[33m la\u001b[0m\u001b[33m fecha\u001b[0m\u001b[33m,\u001b[0m\u001b[33m así\u001b[0m\u001b[33m que\u001b[0m\u001b[33m te\u001b[0m\u001b[33m recom\u001b[0m\u001b[33miendo\u001b[0m\u001b[33m verificar\u001b[0m\u001b[33m con\u001b[0m\u001b[33m f\u001b[0m\u001b[33muentes\u001b[0m\u001b[33m conf\u001b[0m\u001b[33miable\u001b[0m\u001b[33ms\u001b[0m\u001b[33m para\u001b[0m\u001b[33m obtener\u001b[0m\u001b[33m la\u001b[0m\u001b[33m información\u001b[0m\u001b[33m más\u001b[0m\u001b[33m actual\u001b[0m\u001b[33mizada\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m¿\u001b[0m\u001b[33mHay\u001b[0m\u001b[33m algo\u001b[0m\u001b[33m más\u001b[0m\u001b[33m en\u001b[0m\u001b[33m lo\u001b[0m\u001b[33m que\u001b[0m\u001b[33m pueda\u001b[0m\u001b[33m ayud\u001b[0m\u001b[33marte\u001b[0m\u001b[33m?\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "user_prompts = [\n",
    "    \"Quienes son los hijos de Kirk Dougles?\",\n",
    "]\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    session_id = agent.create_session(\"web-session\")\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. \n",
    "        # print(response.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Analysis\n",
    "Here, we can observe that the `builtin::websearch` tool is used to perform a web search. The outputs are displayed in the notebook with color-coded text to help interpret the process:\n",
    "\n",
    "- **Blue Text**: Represents the user's input or query.\n",
    "- **Magenta Text**: Displays the LLM's inference response. \n",
    "- **Pink Text**: Indicates the tool execution process, such as the tool being called and the query being sent to the web search API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- We've demonstrated how to set up Llama Stack agents and extended them with builtin tools (like web search) that come prepackaged with Llama Stack with template Ollama\n",
    "- We've shown that this simple approach can provide significantly increased functionality of existing open source LLM's. \n",
    "- This will serves as a foundational example for the more advanced examples to come involving Agentic RAG, External Tools, and complex agentic patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-stack-client-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
