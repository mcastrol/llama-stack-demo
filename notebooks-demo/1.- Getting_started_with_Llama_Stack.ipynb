{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5ac225-ad97-4cbd-8e90-f0ad64cbf00b",
   "metadata": {},
   "source": [
    "# Getting Started with Llama Stack\n",
    "\n",
    "This notebook is based in the demo published by https://github.com/opendatahub-io/llama-stack-demos/\n",
    "\n",
    "This notebook will help you set up your environment for this tutorial. Specifically, we will cover installing the necessary libraries, configuring essential parameters, and connecting to a Llama Stack server.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589b1ae-9fd5-4715-a630-cc3cc52035e9",
   "metadata": {},
   "source": [
    "## Setting the Environment Variables\n",
    "\n",
    "The environment variables are completed acording to the requirements of Ollama template \n",
    "\n",
    "\n",
    "### Environment variables required for all demos\n",
    "- `MODEL_NAME`: In this case llama3.2:3b-instruct-fp16\n",
    "- `REMOTE_BASE_URL`: the URL of the remote Llama Stack server.(In this case is LOCAL)\n",
    "- `TEMPERATURE` (optional): the temperature to use during inference. Defaults to 0.0.\n",
    "- `TOP_P` (optional): the top_p parameter to use during inference. Defaults to 0.95.\n",
    "- `MAX_TOKENS` (optional): the maximum number of tokens that can be generated in the completion. Defaults to 512.\n",
    "- `STREAM` (optional): set this to True to stream the output of the model/agent and False otherwise. Defaults to False.\n",
    "- `VDB_PROVIDER`: the vector DB provider to be used. Must be supported by Llama Stack. For this demo, we use Faiss which is the default for Ollama\n",
    "- `VDB_EMBEDDING`: the embedding model to be used for ingestion and retrieval. For this demo, we use all-MiniLM-L6-v2.\n",
    "- `VDB_EMBEDDING_DIMENSION` (optional): the dimension of the embedding. Defaults to 384.\n",
    "- `VECTOR_DB_CHUNK_SIZE` (optional): the chunk size for the vector DB. Defaults to 512.\n",
    "- `USE_PROMPT_CHAINING`: dictates if the prompt should be formatted as a few separate prompts to isolate each step or in a single turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573899bf-34cb-4f31-9fc1-48aca439fc60",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b09511e-f6dc-4a54-b037-d64adbeaa8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.types import UserMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15ff12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=os.getenv(\"MODEL_NAME\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180ff092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama3.2:3b-instruct-fp16'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb65ed-f368-4bb5-957b-6698fe85b829",
   "metadata": {},
   "source": [
    "## Setting Up the Server Connection\n",
    "\n",
    "Establish the connection to your Llama Stack server.\n",
    "\n",
    "_Note: A Tavily search API key is required for some of our demos and must be provided to the client upon initialization. If you do not have one, you can set one up for free at https://app.tavily.com_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d60fb3f3-4d04-4916-84fc-f798b059ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n"
     ]
    }
   ],
   "source": [
    "base_url = os.getenv(\"REMOTE_BASE_URL\", \"http://localhost:8321\")\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "\n",
    "print(f\"Connected to Llama Stack server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f5ae8-8083-4896-a973-f310df129ec6",
   "metadata": {},
   "source": [
    "## Initializing the Inference Parameters\n",
    "\n",
    "Fetch the inference-related parameters from the corresponding environment variables and convert them to the format Llama Stack expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0092359-a5db-4d9a-a735-bb931ba05f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Parameters:\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 512}\n",
      "\tstream: False\n"
     ]
    }
   ],
   "source": [
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cdd34a",
   "metadata": {},
   "source": [
    "Now, let's use the Llama stack inference API to greet our LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4423b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = UserMessage(\n",
    "    content=\"Hi, what dd you think about Red Hat?\",\n",
    "    role=\"user\",\n",
    ")\n",
    "client.inference.chat_completion(\n",
    "    model_id=model_name,\n",
    "    messages=[message],\n",
    "    sampling_params=sampling_params,\n",
    "    stream=stream\n",
    ").completion_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0a375",
   "metadata": {},
   "source": [
    "# Next\n",
    "\n",
    "Now that we've set up our Tutorial environment, Let's get started building with Llama Stack! The next notebook will teach you how to build a [Simple RAG](./Level1_simple_RAG.ipynb) application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1219179",
   "metadata": {},
   "source": [
    "#### Any Feedback?\n",
    "\n",
    "If you have any feedback on this or any other notebook in this demo series we'd love to hear it! Please go to https://www.feedback.redhat.com/jfe/form/SV_8pQsoy0U9Ccqsvk and help us improve our demos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-stack-client-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
