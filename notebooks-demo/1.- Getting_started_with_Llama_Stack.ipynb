{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6589b1ae-9fd5-4715-a630-cc3cc52035e9",
   "metadata": {},
   "source": [
    "## Setting the Environment Variables\n",
    "\n",
    "The environment variables are completed acording to the requirements of Ollama template \n",
    "\n",
    "\n",
    "### Environment variables required for all demos\n",
    "- `MODEL_NAME`: In this case llama3.2:3b-instruct-fp16\n",
    "- `REMOTE_BASE_URL`: the URL of the remote Llama Stack server.(In this case is LOCAL)\n",
    "- `TEMPERATURE` (optional): the temperature to use during inference. Defaults to 0.0.\n",
    "- `TOP_P` (optional): the top_p parameter to use during inference. Defaults to 0.95.\n",
    "- `MAX_TOKENS` (optional): the maximum number of tokens that can be generated in the completion. Defaults to 512.\n",
    "- `STREAM` (optional): set this to True to stream the output of the model/agent and False otherwise. Defaults to False.\n",
    "- `VDB_PROVIDER`: the vector DB provider to be used. Must be supported by Llama Stack. For this demo, we use Faiss which is the default for Ollama\n",
    "- `VDB_EMBEDDING`: the embedding model to be used for ingestion and retrieval. For this demo, we use all-MiniLM-L6-v2.\n",
    "- `VDB_EMBEDDING_DIMENSION` (optional): the dimension of the embedding. Defaults to 384.\n",
    "- `VECTOR_DB_CHUNK_SIZE` (optional): the chunk size for the vector DB. Defaults to 512.\n",
    "- `USE_PROMPT_CHAINING`: dictates if the prompt should be formatted as a few separate prompts to isolate each step or in a single turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573899bf-34cb-4f31-9fc1-48aca439fc60",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b09511e-f6dc-4a54-b037-d64adbeaa8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.types import UserMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15ff12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama3.2:3b-instruct-fp16'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name=os.getenv(\"MODEL_NAME\") \n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb65ed-f368-4bb5-957b-6698fe85b829",
   "metadata": {},
   "source": [
    "## Setting Up the Server Connection\n",
    "\n",
    "Establish the connection to your Llama Stack server.\n",
    "\n",
    "_Note: A Tavily search API key is required for some of our demos and must be provided to the client upon initialization. If you do not have one, you can set one up for free at https://app.tavily.com_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60fb3f3-4d04-4916-84fc-f798b059ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n"
     ]
    }
   ],
   "source": [
    "base_url = os.getenv(\"REMOTE_BASE_URL\", \"http://localhost:8321\")\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "\n",
    "print(f\"Connected to Llama Stack server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f5ae8-8083-4896-a973-f310df129ec6",
   "metadata": {},
   "source": [
    "## Initializing the Inference Parameters\n",
    "\n",
    "Fetch the inference-related parameters from the corresponding environment variables and convert them to the format Llama Stack expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0092359-a5db-4d9a-a735-bb931ba05f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Parameters:\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 512}\n",
      "\tstream: True\n"
     ]
    }
   ],
   "source": [
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cdd34a",
   "metadata": {},
   "source": [
    "Now, let's use the Llama stack inference API to greet our LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e4423b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Prepare the message\n",
    "message = UserMessage(\n",
    "    content=\"Hi, what do you know about Red Hat?\",\n",
    "    role=\"user\",\n",
    ")\n",
    "\n",
    "# Call the chat completion and get the response object\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=model_name,\n",
    "    messages=[message],\n",
    "    sampling_params=sampling_params,\n",
    "    timeout=600\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22120cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: Red Hat is a well-known American multinational software company that specializes in open-source software and cloud computing. Here's an overview:\n",
      "\n",
      "**History**: Red Hat was founded in 1993 by Bob Young and Marc Ewing in Santa Clara, California. The company started as a small startup focused on developing and selling Linux operating systems.\n",
      "\n",
      "**Linux and Open-Source**: Red Hat is perhaps best known for its distribution of the Linux operating system, which is an open-source software project that was created by Linus Torvalds in 1991. Red Hat's Enterprise Linux (RHEL) is a popular version of Linux that is widely used in enterprise environments.\n",
      "\n",
      "**Products and Services**: Red Hat offers a range of products and services, including:\n",
      "\n",
      "1. **Red Hat Enterprise Linux (RHEL)**: A commercial version of the Linux operating system designed for enterprise use.\n",
      "2. **Red Hat OpenShift**: An open-source container application platform that allows developers to build, deploy, and manage applications in a cloud-native environment.\n",
      "3. **Red Hat Enterprise Virtualization (RHV)**: A virtualization platform that enables organizations to run multiple operating systems on a single physical server.\n",
      "4. **Red Hat CloudForms**: A cloud management platform that helps organizations manage their cloud infrastructure and applications.\n",
      "5. **Red Hat Ansible**: An automation tool that allows users to automate the deployment, configuration, and management of IT environments.\n",
      "\n",
      "**Acquisitions and Partnerships**: Red Hat has made several strategic acquisitions over the years, including:\n",
      "\n",
      "1. JBoss (2008): A Java-based application server company.\n",
      "2. Gluster (2014): A storage software company.\n",
      "3. OpenShift (2017): A container platform company.\n",
      "4. CoreOS (2018): A Linux distribution and cloud computing company.\n",
      "\n",
      "**Certifications and Recognition**: Red Hat is a leading player in the open-source community, with numerous certifications and recognitions, including:\n",
      "\n",
      "1. **Red Hat Certified Engineer (RHCE)**: A certification program for IT professionals who demonstrate expertise in Linux administration.\n",
      "2. **Red Hat Enterprise Linux Certification**: A certification program that validates an organization's ability to deploy and manage RHEL.\n",
      "3. **Forbes' Best Employers**: Red Hat has been recognized as one of the best employers by Forbes magazine.\n",
      "\n",
      "**Financials**: Red Hat is a publicly traded company (NYSE: RHT) with annual revenues exceeding $2 billion.\n",
      "\n",
      "Overall, Red Hat is a prominent player in the open-source and cloud computing industries, known for its commitment to innovation,\n"
     ]
    }
   ],
   "source": [
    "# Extract the message content and save it\n",
    "answer = response.completion_message.content\n",
    "\n",
    "# Print the content\n",
    "print(\"AI Response:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0a375",
   "metadata": {},
   "source": [
    "# Next\n",
    "\n",
    "Now that we've set up our Tutorial environment, Let's get started building with Llama Stack! The next notebook will teach you how to build a [Simple RAG](./Level1_simple_RAG.ipynb) application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1219179",
   "metadata": {},
   "source": [
    "#### Any Feedback?\n",
    "\n",
    "If you have any feedback on this or any other notebook in this demo series we'd love to hear it! Please go to https://www.feedback.redhat.com/jfe/form/SV_8pQsoy0U9Ccqsvk and help us improve our demos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-stack-client-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
